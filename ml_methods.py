# -*- coding: utf-8 -*-
"""ml_methods.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kcta_4dgwoHNojPjKOe_gX5nyG38Hm9q
"""

#libraries
from sklearn.neighbors import KNeighborsClassifier , NearestNeighbors
from sklearn.metrics import accuracy_score , classification_report
from sklearn import svm 
from sklearn.naive_bayes import GaussianNB
import numpy as np
import cv2
import tensorflow as tf
import matplotlib.pyplot as plt

#class of dataset preprocessing
class dataset_preprocessing:

  #constructor
  def __init__(self):
        self.fashion_mnist = tf.keras.datasets.fashion_mnist
        self.class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
        (self.train_images, self.train_labels), (self.test_images, self.test_labels) = self.fashion_mnist.load_data()
        self.flatt_train_images=[]
        self.flatt_test_images=[]     

  #This function normalize the images of the dataset. The intensity of pixels changes from the range 0-255 to the range 0-1(float).
  def normalization(self):
        self.train_images = self.train_images / 255.0
        self.test_images = self.test_images / 255.0

  #This function reduces the dimensions of initial images. For example if we enter input 80 , we get the 80% of initial image.
  def dimensions_reduce(self):
      scale_percent=float(input("please enter the percent of reduce in the images: "))      
      for i in range(len(self.test_images)):
        width = int(self.test_images[i].shape[1] * scale_percent / 100)
        height = int(self.test_images[i].shape[0] * scale_percent / 100)
        dim = (width, height)
        resized = cv2.resize(self.test_images[i], dim, interpolation = cv2.INTER_AREA)
        self.flatt_test_images.append(resized.flatten())
      for i in range(len(self.train_images)):
        width = int(self.train_images[i].shape[1] * scale_percent / 100)
        height = int(self.train_images[i].shape[0] * scale_percent / 100)
        dim = (width, height)
        resized = cv2.resize(self.train_images[i], dim, interpolation = cv2.INTER_AREA)
        self.flatt_train_images.append(resized.flatten())

#class of machine learning methods
class ml_methods:
  
  #constructor
  def __init__(self,obj):
    self.predicts_test=[]
    self.dataset_obj=obj
    self.flag=0
    self.predictions=[[]]

  #Knn method.Metric can be 'euclidean','manhattan','cosine'.
  def knn(self,metric): 
    self.flag=0
    n=int(input("please enter the number of neighbors: ")) 
    neigh = KNeighborsClassifier(n_neighbors=n,metric=metric)
    neigh.fit(self.dataset_obj.flatt_train_images,self.dataset_obj.train_labels )
    pred_images=np.array(self.dataset_obj.flatt_test_images,dtype='float32')
    self.predicts_test=neigh.predict(pred_images)

  #neural network with 2 hidden layers and the output layer uses the Softmax activation function.
  def neural_network_2hlayers(self):
    self.flag=1
    n=int(input("please enter the number of epochs: ")) 
    model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),tf.keras.layers.Dense(500, activation='sigmoid'),tf.keras.layers.Dense(200, activation='sigmoid'),tf.keras.layers.Dense(10)])
    model.compile(optimizer='SGD',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
    model.fit(self.dataset_obj.train_images, self.dataset_obj.train_labels, epochs=n)
    test_loss, test_acc = model.evaluate(self.dataset_obj.test_images, self.dataset_obj.test_labels, verbose=2)
    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
    self.predictions = probability_model.predict(self.dataset_obj.test_images)

  #support vector machine method. Kernel type can be ‘linear’, ‘poly’, ‘rbf’(Gaussian), ‘sigmoid’, ‘precomputed’ .
  def svm(self,ktype):
    self.flag=0
    clf = svm.SVC(kernel=ktype,decision_function_shape='ovr')
    clf.fit(self.dataset_obj.flatt_train_images,self.dataset_obj.train_labels)
    dec=clf.decision_function(self.dataset_obj.flatt_test_images)
    self.predicts_test=clf.predict(self.dataset_obj.flatt_test_images)

  #bayes method. The least efficient method!
  def bayes(self):
    self.flag=0
    gnb = GaussianNB()
    self.predicts_test = gnb.fit(self.dataset_obj.flatt_train_images,self.dataset_obj.train_labels).predict(self.dataset_obj.flatt_test_images)

  #This function displays variety of metrics and the predictions of every image.
  def visualization(self):
    if self.flag==0:
      print("\n"+classification_report(self.dataset_obj.test_labels,self.predicts_test))
      for i in range(10):
        plt.figure(figsize=(5,5))
        plt.imshow(self.dataset_obj.test_images[i], cmap=plt.cm.binary)
        print('   real: '+self.dataset_obj.class_names[self.dataset_obj.test_labels[i]]+'   pred: '+self.dataset_obj.class_names[self.predicts_test[i]])
        plt.show()
    else:
      for i in range(10):
       plt.figure(figsize=(4,4))
       plt.imshow(self.dataset_obj.test_images[i],cmap='gray')
       print('   real: '+self.dataset_obj.class_names[self.dataset_obj.test_labels[i]]+'   pred: '+self.dataset_obj.class_names[np.argmax(self.predictions[i])])
       plt.show()
       for j in range(10):
          print("propability to be {} is {:2.5f}% ".format(self.dataset_obj.class_names[j],100*self.predictions[i][j]))
       print('-----------------------------------\n\n\n')

images_data=dataset_preprocessing()
images_data.normalization()
images_data.dimensions_reduce()
ml=ml_methods(images_data)
ml.knn('cosine')
ml.visualization()

ml.bayes()
ml.visualization()

ml.neural_network_2hlayers()
ml.visualization()

ml.svm('rbf')
ml.visualization()